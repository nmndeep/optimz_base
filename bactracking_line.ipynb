{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    Naman Deep Singh, Backtracking line search\n",
    "    for Gradient descent on a one layer-linear classifier for MNIST data.\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def backprop(X, y, W, lamb, n_classes, batch_size):\n",
    "    F_x = np.dot(X,W)\n",
    "\n",
    "    C = np.max(F_x, axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(F_x - C)\n",
    "\n",
    "    S = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    #  log-loss\n",
    "    log_S = -np.log(S[np.arange(batch_size), y])\n",
    "    L = np.sum(log_S)/batch_size\n",
    "\n",
    "    # Add regularization using the L2 norm\n",
    "    L_reg = 0.5*lamb*np.sum(W*W)\n",
    "    L+= L_reg\n",
    "\n",
    "    # Gradient of the loss with respect to scores\n",
    "    grad = S.copy()\n",
    "    # Substract 1 from the scores of the correct class\n",
    "    grad[np.arange(batch_size),y] -= 1\n",
    "    grad /= batch_size\n",
    "\n",
    "    # Gradient of the loss with respect to weights\n",
    "    grad_W = X.T.dot(grad)\n",
    "\n",
    "    # Add gradient regularization\n",
    "    grad_W+= lamb*W\n",
    "\n",
    "    return L, grad_W\n",
    "\n",
    "                                #########     Function for backtracking rule        ############\n",
    "######   X = train data, y= train labels    W = weights\n",
    "def backtrack(W, grad_W, loss, alpha, X, y, lamb, t, beta, n_classes, batch_size):\n",
    "    count = 1\n",
    "    loss_val,_ = backprop(X, y, W-t*grad_W, lamb, n_classes, batch_size)    #####     Function value at x-t*x'\n",
    "    while loss_val <= loss-(t*alpha)*np.square(np.linalg.norm(grad_W, 'fro')):              ########      f(x-tx') > f(x) - t*alpha*|x'|^2\n",
    "        t *= beta\n",
    "        loss_val,_ = backprop(X, y, W-t*grad_W, lamb, n_classes, batch_size)\n",
    "\n",
    "    return t\n",
    "\n",
    "grad_W = np.zeros_like(W)\n",
    "                    #################      Condition either full run of training data or ||grad_W||<1e-3      ##########\n",
    "def grad_descent(X, y, lamb, grad_W, W, n_classes):\n",
    "    # W = np.zeros(shape=(784, n_classes))\n",
    "    batch_size = 100\n",
    "    lamb = 1e-2   ### lambda in weight regularizer\n",
    "    t = 1\n",
    "    alpha = 0.4\n",
    "    beta = 0.6      ###    For backtracking\n",
    "    while np.linalg.norm(grad_W, 'fro') < 1e-3:\n",
    "        start = 0\n",
    "        while start < number_of_samples-batch_size :\n",
    "            loss, grad_W = backprop(X[start:start+batch_size,:], y[start:start+batch_size], W, lamb, n_classes, batch_size)\n",
    "            alpha = backtrack(W, grad_W, loss, alpha, X[start:start+batch_size,:], y[start:start+batch_size], lamb, t, beta, n_classes, batch_size)\n",
    "            # count+=1\n",
    "            t = alpha\n",
    "            W = W - alpha*grad_W\n",
    "            start+=batch_size\n",
    "    return W\n",
    "\n",
    "Weights = grad_descent(X, y, grad_W, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
